
# coding: utf-8

# In[44]:


import requests
res = requests.get('http://news.sina.com.cn/china/')
res.encoding = 'utf-8'
print(type(res))
print(res.text)


# In[37]:


from bs4 import BeautifulSoup
html_sample = ' <html> <body> <h1 id = "title"> Hello World </h1> <a href="a" class="link"> This is link1 </a><a href="a" class="link"> This is link2 </a> </body> </html>'

soup = BeautifulSoup(html_sample,'html.parser')
print(type(soup))
print(soup.text)


# In[39]:


header = soup.select('h1')
print(header)


# In[69]:


alink = soup.select('a')
#print(alink[0].text)
print(alink[0]['href'])
for link in alink:
    print(link['href'])


# In[84]:


sina = BeautifulSoup(res.text,'html.parser')
content = sina.select('.news-item')
for title in content:
    if len(title.select('h2')) > 0:
        h2 = title.select('h2')[0].text
        a  = title.select('a')[0]['href']
        time = title.select('.time')[0].text
        print(h2,a,time)


# In[299]:


res2 = requests.get('http://news.sina.com.cn/c/nd/2018-02-22/doc-ifyrvaxe8717342.shtml')
res2.encoding = 'utf-8'
page = BeautifulSoup(res2.text,'html.parser')
tit3 = page.select('.main-title')[0].text
date = page.select('.date')[0].text
source = page.select('.source')[0].text
print(tit3)
date


# In[124]:


from datetime import datetime
dt = datetime.strptime(date,'%Y年%m月%d日 %H:%M')
dt


# In[125]:


dt.strftime('%Y-%m-%d')


# In[143]:


xx = page.select('.date-source a')[0]['href']
xx


# In[306]:


def getart():
    art = []
    for p in page.select('#article p')[:-1]:
        art.append(p.text.strip())
    artic = '/n'.join(art)
    return artic
getart()


# In[161]:


editor = page.select('.show_author')[0].text.lstrip('责任编辑：').strip()
editor


# In[273]:


com = requests.get('http://comment5.news.sina.com.cn/page/info?version=1&format=json&channel=gn&newsid=comos-fyrvaxe8717342&group=undefined&compress=0&ie=utf-8&oe=utf-8&page=1&page_size=3&t_size=3&h_size=3&thread=1')
#print(com.text)
import json
jd =  json.loads(com.text)
jd['result']['count']['total']


# In[198]:


newsurl = 'http://news.sina.com.cn/c/nd/2018-02-22/doc-ifyrvaxe8717342.shtml'
newsid = newsurl.split('/')[-1].lstrip('doc-i').rstrip('.shtml')
newsid


# In[202]:


import re
m = re.search('doc-i(.+).shtml',newsurl)
newsid = m.group(1)
newsid


# In[214]:


curl = 'http://comment5.news.sina.com.cn/page/info?version=1&format=json&channel=gn&newsid=comos-{}&group=undefined&compress=0&ie=utf-8&oe=utf-8&page=1&page_size=3&t_size=3&h_size=3&thread=1'
curl.format(newsid)


# In[297]:


def getcomment(url):
    m = re.search('doc-i(.+).shtml',url)
    newsid = m.group(1)
    ccurl = 'http://comment5.news.sina.com.cn/page/info?version=1&format=json&channel=gn&newsid=comos-{}&group=undefined&compress=0&ie=utf-8&oe=utf-8&page=1&page_size=3&t_size=3&h_size=3&thread=1'
    courl = ccurl.format(newsid)
    com = requests.get(courl)
    jd =  json.loads(com.text.strip())
    return jd['result']['count']['total']


# In[298]:


uurl = 'http://news.sina.com.cn/c/nd/2018-02-22/doc-ifyrvaxe8717342.shtml'
getcomment(uurl)


# In[307]:


def getart():
    art = []
    for p in page.select('#article p')[:-1]:
        art.append(p.text.strip())
    artic = '/n'.join(art)
    return artic
getart()


# In[311]:


def getnewsdetail(news):
    result = {}
    newspage = requests.get(news)
    newspage.encoding = 'utf-8'
    content = BeautifulSoup(newspage.text,'html.parser')
    result['title'] = content.select('.main-title')[0].text
    result['date'] = content.select('.date')[0].text
    result['source'] = content.select('.source')[0].text
    art = []
    for p in content.select('#article p')[:-1]:
        art.append(p.text.strip())
    artic = '/n'.join(art)
    result['article'] = artic
    result['editor'] = content.select('.show_author')[0].text.lstrip('责任编辑：').strip()
    result['comments'] = getcomment(news)
    return result


# In[312]:


getnewsdetail('http://news.sina.com.cn/c/2018-02-23/doc-ifyrvaxe9214673.shtml')


# In[320]:


arls = requests.get('http://api.roll.news.sina.com.cn/zt_list?channel=news&cat_1=gnxw&cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&level==1||=2&show_ext=1&show_all=1&show_num=22&tag=1&format=json&page=2&callback=newsloadercallback&_=1519402396976')
are = json.loads(arls.text.lstrip('  newsloadercallback(').rstrip(');'))
#are['result']['data']


# In[323]:


for ite in are['result']['data']:
    print(ite['url'])


# In[331]:


def getarls(url):
    newsls = []
    arls = requests.get(url)
    are = json.loads(arls.text.lstrip('  newsloadercallback(').rstrip(');'))
    for ite in are['result']['data']:
        newsls.append(getnewsdetail(ite['url']))
    return newsls


# In[333]:


teurl = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&cat_1=gnxw&cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&level==1||=2&show_ext=1&show_all=1&show_num=22&tag=1&format=json&page=2&callback=newsloadercallback&_=1519402396976'
getarls(teurl)


# In[341]:


tesurl = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&cat_1=gnxw&cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&level==1||=2&show_ext=1&show_all=1&show_num=22&tag=1&format=json&page={}&callback=newsloadercallback&_=1519402396976'
for i in range(1,10):
    print(tesurl.format(i))


# In[340]:


tesurl = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&cat_1=gnxw&cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&level==1||=2&show_ext=1&show_all=1&show_num=22&tag=1&format=json&page={}&callback=newsloadercallback&_=1519402396976'
teurlls = []
for i in range(1,10):
    teurlls.append(tesurl.format(i).strip())
print(teurlls)


# In[343]:


for ied in teurlls:
    print(ied)


# In[346]:


tesurl = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&cat_1=gnxw&cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&level==1||=2&show_ext=1&show_all=1&show_num=22&tag=1&format=json&page={}&callback=newsloadercallback&_=1519402396976'
newstotal = []
for i in range(1,3):
    vurl = tesurl.format(i)
    temcon = getarls(vurl)
    newstotal.extend(temcon)
print(newstotal)


# In[345]:


len(newstotal)


# In[349]:


import pandas
df = pandas.DataFrame(newstotal)
df.head()


# In[350]:


df.to_excel('news.xlsx')


# In[352]:


import sqlite3
with sqlite3.connect('news.sqlite') as db:
    df.to_sql('news', con = db)


# In[353]:


with sqlite3.connect('news.sqlite') as db:
    df2 = pandas.read_sql_query('SELECT * FROM news', con = db)
df2

